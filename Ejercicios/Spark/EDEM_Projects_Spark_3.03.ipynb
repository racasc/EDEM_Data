{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EDEM_Projects_Spark.ipynb","provenance":[],"collapsed_sections":["Jq9d0x1OTh2N"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Jq9d0x1OTh2N"},"source":["# Prerrequisites"]},{"cell_type":"markdown","metadata":{"id":"2_DQBVj_KNvL"},"source":["Installing Spark and Apache Kafka Library in VM\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"xEbGSM3_NM-z"},"source":["!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n","!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n","!pip -q install findspark"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gt5sHazLNzqb"},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.8.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog pyspark-shell'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TP_HtvSAj4sI"},"source":["import findspark\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_IgZEv2XaDm"},"source":["Starting Spark Session and print the version\n","\n","\n","---\n"]},{"cell_type":"code","metadata":{"id":"BDLMbVBATf9K"},"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *\n","\n","# create the session\n","spark = SparkSession \\\n","        .builder \\\n","        .master(\"local[*]\") \\\n","        .config(\"spark.ui.port\", \"4050\") \\\n","        .getOrCreate()\n","\n","spark.version"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNiYuI5dGo8Y"},"source":["Creating ngrok tunnel to allow Spark UI (Optional)\n"]},{"cell_type":"code","metadata":{"id":"x4-7fXZiGmqB"},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","get_ipython().system_raw('./ngrok http 4050 &')\n","!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ruHocwYcT4aj"},"source":["# Download Datasets"]},{"cell_type":"code","metadata":{"id":"musWXLzBUEQg"},"source":["!mkdir -p /dataset\n","!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.csv -P /dataset\n","!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/trades.json -P /dataset\n","!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.edges.csv -P /dataset\n","!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.address.csv -P /dataset\n","!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.intermediary.csv -P /dataset\n","!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.officer.csv -P /dataset\n","!wget -q https://github.com/masfworld/datahack_docker/raw/master/zeppelin/data/offshore_leaks.nodes.entity.csv -P /dataset\n","!ls /dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42PI1onm9kIh"},"source":["# Project 1 - Regulatory Banking Project\n","---\n","\n","Input files: /dataset/trades.csv & /dataset/trades.json\n"]},{"cell_type":"code","metadata":{"id":"ebjNK66MgA_i"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1h6LICd9ZBPw"},"source":["# Project 2 - Transactions Notifications\n","\n","*Hint: https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html*"]},{"cell_type":"code","metadata":{"id":"49tIaJ7gZN2C"},"source":["from pyspark.sql.functions import from_json, col\n","from pyspark.sql.types import StructType, StructField, StringType\n","\n","df = spark \\\n","  .readStream \\\n","  .format(\"kafka\") \\\n","  .option(\"kafka.bootstrap.servers\", \"ec2-3-231-22-58.compute-1.amazonaws.com:9092\") \\\n","  .option(\"subscribe\", \"transactions\") \\\n","  .load()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6SLWzYRZbud"},"source":["schema = StructType(\n","    [\n","     StructField('Account No', StringType(), True),\n","     StructField('DATE', StringType(), True),\n","     StructField('TRANSACTION DETAILS', StringType(), True),\n","     StructField('CHQ.NO.', StringType(), True),\n","     StructField('VALUE DATE', StringType(), True),\n","     StructField(' WITHDRAWAL AMT ', StringType(), True),\n","     StructField(' DEPOSIT AMT ', StringType(), True),\n","     StructField('BALANCE AMT', StringType(), True)\n","    ]\n",")\n","df.printSchema()\n","\n","dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\n","    .withColumn(\"value\", from_json(\"value\", schema)) \\\n","    .select(col('key'), col(\"timestamp\"), col('value.*'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eh82Gqz7Z1bm"},"source":["dataset_count.writeStream \\\n"," .outputMode(\"update\") \\\n"," .format(\"memory\") \\\n"," .option(\"truncate\", \"false\") \\\n"," .queryName(\"transactions\") \\\n"," .start()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2KkvViQaXKj"},"source":["spark.sql(\"select * from transactions\").show(truncate = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yG8_q4f5Q_E6"},"source":["# Project 3 - Panama Papers"]},{"cell_type":"markdown","metadata":{"id":"NApu7049VPwm"},"source":["Trace \"Spring Song International Co., Ltd.\" entity with Spark SQL using the following dataset</br>\n","/dataset/offshore_leaks.nodes.entity.csv </br>\n","/dataset/offshore_leaks.nodes.intermediary.csv </br>\n","/dataset/offshore_leaks.edges.csv </br>\n","/dataset/offshore_leaks.nodes.officer.csv"]},{"cell_type":"code","metadata":{"id":"9ydUZNSrYGyN"},"source":[""],"execution_count":null,"outputs":[]}]}